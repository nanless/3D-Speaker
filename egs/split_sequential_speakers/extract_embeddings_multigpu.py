#!/usr/bin/env python3
# Copyright 3D-Speaker (https://github.com/alibaba-damo-academy/3D-Speaker). All Rights Reserved.
# Licensed under the Apache License, Version 2.0 (http://www.apache.org/licenses/LICENSE-2.0)

"""
åŸºäº3dspeakeræ¨¡å‹çš„å¤šGPU embeddingæå–è„šæœ¬
æ”¯æŒå¤šGPUå¹¶è¡Œå¤„ç†ï¼Œç”¨äºå¤§è§„æ¨¡éŸ³é¢‘æ•°æ®çš„embeddingæå–
"""

import os
import sys
import json
import pickle
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.utils.data import Dataset, DataLoader
import torchaudio
import numpy as np
import argparse
from pathlib import Path
from tqdm import tqdm
import time
import warnings
from concurrent.futures import ThreadPoolExecutor
import logging
from typing import List, Dict, Tuple, Optional

# æ·»åŠ speakerlabè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
from speakerlab.utils.builder import dynamic_import

warnings.filterwarnings('ignore')

def setup_logging(log_file=None):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    log_format = '%(asctime)s - %(levelname)s - %(message)s'
    if log_file:
        logging.basicConfig(level=logging.INFO, format=log_format, 
                          handlers=[
                              logging.FileHandler(log_file),
                              logging.StreamHandler()
                          ])
    else:
        logging.basicConfig(level=logging.INFO, format=log_format)
    return logging.getLogger(__name__)

def setup(rank, world_size, port='12355'):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = str(port)
    
    print(f"ğŸ”— GPU {rank} è¿æ¥åˆ°åˆ†å¸ƒå¼ç»„ï¼Œç«¯å£: {port}")
    
    try:
        dist.init_process_group("nccl", rank=rank, world_size=world_size)
        print(f"âœ… GPU {rank} NCCLåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ GPU {rank} NCCLåˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°glooåç«¯: {e}")
        try:
            dist.init_process_group("gloo", rank=rank, world_size=world_size)
            print(f"âœ… GPU {rank} GLOOåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e2:
            print(f"âŒ GPU {rank} åˆ†å¸ƒå¼åˆå§‹åŒ–å®Œå…¨å¤±è´¥: {e2}")
            raise

def cleanup():
    """æ¸…ç†åˆ†å¸ƒå¼è¿›ç¨‹ç»„"""
    dist.destroy_process_group()

def scan_audio_files(input_dir):
    """æ‰«æéŸ³é¢‘æ–‡ä»¶"""
    audio_extensions = ['.wav', '.flac', '.mp3', '.m4a']
    audio_files = []
    
    input_path = Path(input_dir)
    if not input_path.exists():
        raise ValueError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
    
    print(f"ğŸ” æ‰«æéŸ³é¢‘æ–‡ä»¶: {input_dir}")
    
    for ext in audio_extensions:
        files = list(input_path.rglob(f'*{ext}'))
        audio_files.extend(files)
    
    # æŒ‰è·¯å¾„æ’åºç¡®ä¿é¡ºåºä¸€è‡´
    audio_files.sort()
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
    return [str(f) for f in audio_files]

class AudioDataset(Dataset):
    """éŸ³é¢‘æ•°æ®é›†"""
    
    def __init__(self, audio_files: List[str], target_sr: int = 16000):
        self.audio_files = audio_files
        self.target_sr = target_sr
    
    def __len__(self):
        return len(self.audio_files)
    
    def __getitem__(self, idx):
        audio_file = self.audio_files[idx]
        
        try:
            # åŠ è½½éŸ³é¢‘
            waveform, sr = torchaudio.load(audio_file)
            
            # è½¬æ¢ä¸ºå•å£°é“
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # é‡é‡‡æ ·
            if sr != self.target_sr:
                resampler = torchaudio.transforms.Resample(sr, self.target_sr)
                waveform = resampler(waveform)
            
            # è·å–æ—¶é•¿
            duration = waveform.shape[1] / self.target_sr
            
            return {
                'waveform': waveform.squeeze(0),  # ä¿æŒåŸå§‹é•¿åº¦
                'audio_file': audio_file,
                'original_sr': sr,
                'duration': duration,
                'samples': waveform.shape[1]
            }
            
        except Exception as e:
            print(f"âŒ åŠ è½½éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {audio_file}, é”™è¯¯: {e}")
            return {
                'waveform': torch.zeros(8000),  # 0.5ç§’çš„å ä½ç¬¦
                'audio_file': audio_file,
                'original_sr': self.target_sr,
                'duration': 0.0,
                'samples': 0,
                'error': str(e)
            }

def load_3dspeaker_model(device: str):
    """åŠ è½½3dspeakeräº‘ç«¯æ¨¡å‹"""
    print(f"ğŸ¯ åŠ è½½3dspeakeräº‘ç«¯æ¨¡å‹...")
    
    try:
        from modelscope.pipelines import pipeline
        
        # ç›´æ¥ä½¿ç”¨modelscopeäº‘ç«¯æ¨¡å‹
        inference_pipeline = pipeline(
            task='speaker-verification',
            model='iic/speech_eres2netv2w24s4ep4_sv_zh-cn_16k-common',
            model_revision='v1.0.1',
            device=device
        )
        
        print(f"âœ… æˆåŠŸåŠ è½½modelscopeäº‘ç«¯æ¨¡å‹åˆ°è®¾å¤‡: {device}")
        return inference_pipeline, 'modelscope'
        
    except Exception as e:
        print(f"âŒ åŠ è½½modelscopeäº‘ç«¯æ¨¡å‹å¤±è´¥: {e}")
        raise ValueError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

def extract_single_embedding(model, model_type: str, waveform: torch.Tensor, audio_data: Dict, device: str):
    """æå–å•ä¸ªéŸ³é¢‘çš„embedding"""
    try:
        if model_type == 'modelscope':
            # è½¬æ¢ä¸ºnumpy
            audio_numpy = waveform.cpu().numpy()
            
            # ç¡®ä¿éŸ³é¢‘é•¿åº¦ä¸å¤ªçŸ­ï¼ˆè‡³å°‘0.1ç§’ï¼‰
            if len(audio_numpy) < int(0.1 * 16000):
                # å¡«å……åˆ°0.5ç§’
                min_length = int(0.5 * 16000)
                audio_numpy = np.pad(audio_numpy, (0, max(0, min_length - len(audio_numpy))), mode='constant')
            
            # å°è¯•ç›´æ¥è°ƒç”¨modelçš„forwardæ–¹æ³•è·å–embedding
            if hasattr(model, 'model') and hasattr(model.model, 'forward'):
                # å°†éŸ³é¢‘è½¬æ¢ä¸ºtensorå¹¶ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
                audio_tensor = torch.from_numpy(audio_numpy).unsqueeze(0).to(device)
                with torch.no_grad():
                    # ç›´æ¥è°ƒç”¨æ¨¡å‹forwardæ–¹æ³•è·å–embedding
                    result = model.model.forward(audio_tensor)
                    if isinstance(result, dict) and 'emb' in result:
                        embedding = result['emb']
                    elif isinstance(result, torch.Tensor):
                        embedding = result
                    else:
                        embedding = result[0] if isinstance(result, (list, tuple)) else result
            else:
                # å›é€€åˆ°ä½¿ç”¨pipelineçš„æ–¹å¼
                # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„ç¬¬äºŒä¸ªéŸ³é¢‘ï¼ˆè‡ªå·±å’Œè‡ªå·±æ¯”è¾ƒï¼‰æ¥è§¦å‘embeddingæå–
                result = model([audio_numpy, audio_numpy])
                
                # å°è¯•ä»ç»“æœä¸­æå–embedding
                if hasattr(model, 'model') and hasattr(model.model, 'extract_emb'):
                    embedding = model.model.extract_emb(torch.from_numpy(audio_numpy).unsqueeze(0).to(device))
                else:
                    # å¦‚æœæ— æ³•ç›´æ¥è·å–embeddingï¼Œä½¿ç”¨ä¸€ä¸ªé»˜è®¤æ–¹æ³•
                    if hasattr(model, 'model'):
                        audio_tensor = torch.from_numpy(audio_numpy).unsqueeze(0).to(device)
                        with torch.no_grad():
                            embedding = model.model(audio_tensor)
                            if isinstance(embedding, dict):
                                embedding = embedding.get('emb', embedding.get('embedding', list(embedding.values())[0]))
                    else:
                        embedding = np.zeros(192)  # é»˜è®¤ç»´åº¦ (eres2netv2 æ˜¯192ç»´)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            if torch.is_tensor(embedding):
                embedding = embedding.cpu().numpy()
            elif isinstance(embedding, list):
                embedding = np.array(embedding)
            
            # ç¡®ä¿æ˜¯1ç»´æ•°ç»„
            if embedding.ndim > 1:
                embedding = embedding.flatten()
            
            # æ£€æŸ¥embeddingæ˜¯å¦æœ‰æ•ˆ
            if len(embedding) == 0:
                embedding = np.zeros(192)
            
            return embedding
            
        else:
            # torchæ¨¡å‹å¤„ç†ï¼ˆéœ€è¦å…·ä½“å®ç°ï¼‰
            return np.zeros(192)
    
    except Exception as e:
        print(f"âŒ æå–embeddingå¤±è´¥: {audio_data.get('audio_file', 'unknown')}, é”™è¯¯: {e}")
        return np.zeros(192)

def extract_embeddings_on_gpu(rank, world_size, args, audio_files):
    """åœ¨æŒ‡å®šGPUä¸Šæå–embeddings"""
    setup(rank, world_size, args.port)
    
    device = f"cuda:{rank}"
    torch.cuda.set_device(rank)
    
    # ç®€åŒ–æ—¥å¿—ï¼Œä¸åˆ›å»ºæ–‡ä»¶
    logger = setup_logging()
    
    print(f"ğŸš€ GPU {rank} å¼€å§‹å¤„ç† {len(audio_files)} ä¸ªæ–‡ä»¶...")
    
    # åˆ†é…æ–‡ä»¶åˆ°å½“å‰GPU
    files_per_gpu = len(audio_files) // world_size
    start_idx = rank * files_per_gpu
    if rank == world_size - 1:
        end_idx = len(audio_files)  # æœ€åä¸€ä¸ªGPUå¤„ç†å‰©ä½™æ‰€æœ‰æ–‡ä»¶
    else:
        end_idx = start_idx + files_per_gpu
    
    gpu_audio_files = audio_files[start_idx:end_idx]
    print(f"ğŸ¯ GPU {rank} å¤„ç†æ–‡ä»¶èŒƒå›´: [{start_idx}:{end_idx}] ({len(gpu_audio_files)} ä¸ªæ–‡ä»¶)")
    
    # åŠ è½½æ¨¡å‹
    try:
        model, model_type = load_3dspeaker_model(device)
        print(f"âœ… GPU {rank} æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ GPU {rank} æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        cleanup()
        return
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = AudioDataset(gpu_audio_files, target_sr=16000)
    
    # å¤„ç†ç»Ÿè®¡
    processed_count = 0
    error_count = 0
    start_time = time.time()
    
    # é€æ¡å¤„ç†éŸ³é¢‘æ–‡ä»¶å¹¶ç›´æ¥ä¿å­˜
    with torch.no_grad():
        for file_idx in tqdm(range(len(dataset)), desc=f"GPU {rank} å¤„ç†ä¸­", disable=(rank != 0)):
            try:
                # è·å–å•ä¸ªéŸ³é¢‘æ•°æ®
                audio_data = dataset[file_idx]
                audio_file = audio_data['audio_file']
                
                # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                if 'error' in audio_data:
                    if rank == 0:
                        print(f"âš ï¸ éŸ³é¢‘æ–‡ä»¶æœ‰é”™è¯¯: {audio_file}, é”™è¯¯: {audio_data['error']}")
                    error_count += 1
                    continue
                
                # ç§»åŠ¨æ•°æ®åˆ°GPU
                waveform = audio_data['waveform'].to(device)
                
                # æå–å•ä¸ªembedding
                embedding = extract_single_embedding(model, model_type, waveform, audio_data, device)
                
                # åˆ›å»ºembeddingæ•°æ®
                embedding_data = {
                    'embedding': embedding,
                    'audio_file': audio_file,
                    'original_path': audio_file,
                    'relative_path': os.path.relpath(audio_file, args.input_dir),
                    'filename': os.path.basename(audio_file),
                    'duration': audio_data['duration'],
                    'samples': audio_data.get('samples', 0),
                    'sample_rate': 16000,
                    'embedding_dim': len(embedding),
                    'model_type': model_type
                }
                
                # ç›´æ¥ä¿å­˜åˆ°å¯¹åº”ä½ç½®
                relative_path = embedding_data['relative_path']
                file_base = os.path.splitext(relative_path)[0]
                pkl_file_path = os.path.join(args.output_dir, f"{file_base}.pkl")
                
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                pkl_dir = os.path.dirname(pkl_file_path)
                if pkl_dir:
                    os.makedirs(pkl_dir, exist_ok=True)
                
                # ä¿å­˜å•ä¸ªembeddingæ–‡ä»¶
                with open(pkl_file_path, 'wb') as f:
                    pickle.dump(embedding_data, f)
                
                processed_count += 1
                
            except Exception as e:
                if rank == 0:
                    print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {gpu_audio_files[file_idx] if file_idx < len(gpu_audio_files) else 'unknown'}, é”™è¯¯: {e}")
                error_count += 1
                continue
    
    # å¤„ç†å®Œæˆç»Ÿè®¡
    processing_time = time.time() - start_time
    
    if rank == 0:
        print(f"ğŸ‰ GPU {rank} å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {processed_count}/{len(gpu_audio_files)} æˆåŠŸ, {error_count} é”™è¯¯")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        print(f"ğŸš€ å¤„ç†é€Ÿåº¦: {processed_count/processing_time:.2f} æ–‡ä»¶/ç§’")
    
    cleanup()


def collect_final_stats(output_dir: str, audio_files: List[str], input_dir: str):
    """ç»Ÿè®¡æœ€ç»ˆå¤„ç†ç»“æœ"""
    print("ğŸ“Š ç»Ÿè®¡å¤„ç†ç»“æœ...")
    
    processed_count = 0
    failed_count = 0
    
    for audio_file in audio_files:
        relative_path = os.path.relpath(audio_file, input_dir)
        file_base = os.path.splitext(relative_path)[0]
        pkl_file_path = os.path.join(output_dir, f"{file_base}.pkl")
        
        if os.path.exists(pkl_file_path):
            processed_count += 1
        else:
            failed_count += 1
    
    success_rate = processed_count / len(audio_files) * 100 if audio_files else 0
    
    print(f"âœ… å¤„ç†å®Œæˆ!")
    print(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {len(audio_files)}")
    print(f"âœ… æˆåŠŸå¤„ç†: {processed_count}")
    print(f"âŒ å¤„ç†å¤±è´¥: {failed_count}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.2f}%")
    
    return {
        'total_files': len(audio_files),
        'processed_count': processed_count,
        'failed_count': failed_count,
        'success_rate': success_rate
    }

def main():
    parser = argparse.ArgumentParser(description="3dspeakerå¤šGPU embeddingæå–")
    parser.add_argument('--input_dir', type=str, default="/root/group-shared/voiceprint/data/speech/speech_enhancement/child-2.07M/wav_files",
                       help='è¾“å…¥éŸ³é¢‘æ–‡ä»¶ç›®å½•')
    parser.add_argument('--output_dir', type=str, default="/root/group-shared/voiceprint/data/speech/speech_enhancement/child-2.07M/wav_embeddings_eres2netv2w24s4ep4",
                       help='è¾“å‡ºembeddingç›®å½•')
    parser.add_argument('--world_size', type=int, default=4,
                       help='GPUæ•°é‡ï¼ˆé»˜è®¤4ï¼‰')
    parser.add_argument('--port', type=str, default='12355',
                       help='åˆ†å¸ƒå¼é€šä¿¡ç«¯å£ï¼ˆé»˜è®¤12355ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(args.input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    print(f"ğŸ¯ å¼€å§‹å¤šGPU embeddingæå–")
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸŒ ä½¿ç”¨modelscopeäº‘ç«¯æ¨¡å‹: iic/speech_eres2netv2w24s4ep4_sv_zh-cn_16k-common")
    print(f"ğŸ® GPUæ•°é‡: {args.world_size}")
    print(f"ğŸµ ä¿æŒåŸå§‹éŸ³é¢‘é•¿åº¦ï¼Œä¸æˆªæ–­ä¸å¡«å……")
    print(f"ğŸ’¾ æ¯ä¸ªæ–‡ä»¶å¤„ç†å®Œæˆåç›´æ¥ä¿å­˜åˆ°å¯¹åº”ä½ç½®")
    print("")
    
    start_time = time.time()
    
    # æ‰«æéŸ³é¢‘æ–‡ä»¶
    audio_files = scan_audio_files(args.input_dir)
    
    if not audio_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
        return
    
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•ä»¥æ”¯æŒCUDA
    mp.set_start_method('spawn', force=True)
    
    # åœ¨ä¸»è¿›ç¨‹ä¸­æ‰¾åˆ°å¯ç”¨ç«¯å£
    import socket
    def find_free_port():
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('', 0))
            s.listen(1)
            port = s.getsockname()[1]
        return port
    
    # æ£€æŸ¥æŒ‡å®šç«¯å£æ˜¯å¦å¯ç”¨
    master_port = args.port
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.bind(('localhost', int(master_port)))
        print(f"ğŸ”Œ ä½¿ç”¨æŒ‡å®šç«¯å£: {master_port}")
    except OSError:
        original_port = master_port
        master_port = str(find_free_port())
        print(f"âš ï¸ ç«¯å£ {original_port} è¢«å ç”¨ï¼Œä½¿ç”¨åŠ¨æ€ç«¯å£: {master_port}")
    
    # æ›´æ–°argsä¸­çš„ç«¯å£
    args.port = master_port
    
    # å¯åŠ¨å¤šè¿›ç¨‹å¤„ç†
    print(f"ğŸš€ å¯åŠ¨ {args.world_size} ä¸ªGPUè¿›ç¨‹...")
    mp.spawn(extract_embeddings_on_gpu,
             args=(args.world_size, args, audio_files),
             nprocs=args.world_size,
             join=True)
    
    # ç»Ÿè®¡æœ€ç»ˆç»“æœ
    final_stats = collect_final_stats(args.output_dir, audio_files, args.input_dir)
    
    total_time = time.time() - start_time
    print(f"\nğŸ‰ å¤šGPU embeddingæå–å®Œæˆ!")
    print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)")
    print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {final_stats['processed_count']/total_time:.2f} æ–‡ä»¶/ç§’")
    print(f"ğŸ’¾ ç»“æœç›´æ¥ä¿å­˜åˆ°: {args.output_dir}")
    print(f"ğŸ“‚ embeddingæ–‡ä»¶ä¸åŸéŸ³é¢‘æ–‡ä»¶ä¿æŒç›¸åŒçš„ç›®å½•ç»“æ„å’Œæ–‡ä»¶å")

if __name__ == "__main__":
    main()